import prefs
from bot_instance import client, bot
import datetime
from stuff import *
import json
from stuff import *
from google.genai import types
import tools_package.tools as tools


def convert_conversation():
    google_conversation = []
    with open("static_storage/conversation.json", "r", encoding="utf-8") as f:
        conversation = json.load(f)
    for turn in conversation:
        google_conversation.append(types.Content(
            role=turn["role"],
            parts=[
                types.Part.from_text(text=turn["content"]),
            ]
        ))
    return google_conversation


def convert_single(message:str):
    return types.Content(
            role="user",
            parts=[
                types.Part.from_text(text=message),
            ]
        )


def convert_single_as_function(message:str):
    return types.Content(
            role="model",
            parts=[
                types.Part.from_text(text=message),
            ]
        )
        

def raw_conversation():
    with open("static_storage/conversation.json", "r", encoding="utf-8") as f:
        conversation = json.load(f)
    return conversation



def smart_response(
    TOOLSET=tools.G_TOOLS, 
    TEMP=prefs.TEMPERATURE, 
    system_message:types.Part=None, 
    func_mode="AUTO",
    messages=None
):
    """
    Args:
        func_mode: The mode for function calling. Must be one of "ANY", "AUTO", or "NONE".
                    - "ANY": Forces the model to predict only function calls.
                    - "AUTO": Lets the model decide between function calls and text responses.
                    - "NONE": Disables function calling entirely.
                    Defaults to "AUTO".
"""
    
    try:
        print(f"{GREEN}Smart message launched{RESET}")
        current_datetime = datetime.datetime.now(prefs.timezone).strftime('%D-%M-%Y %H:%M:%S %Z')
        if not system_message:
            system_message = types.Part.from_text(text=f"Current time is {current_datetime} {prefs.system_msg()}")   
            
        print("sys message created")
        
        tool_config = types.ToolConfig(
            function_calling_config=types.FunctionCallingConfig(
                mode=func_mode,
                # allowed_function_names=["get_current_weather"],
            )
        )
        print("tool config created")
        
        generate_content_config = types.GenerateContentConfig(
            temperature=TEMP,
            top_p=0.95,
            top_k=40,
            max_output_tokens=8192,
            tools=TOOLSET,
            response_mime_type="text/plain",
            system_instruction=[
                system_message
            ],
            tool_config=tool_config
        )
        print("config created")
        if messages is None:
            messages = convert_conversation()
            
            
        raw_messages = raw_conversation()
        print("messages loaded")
        
        plain_text = ""
        function_calls = []
        response = client.models.generate_content_stream(
            model=prefs.model_gemini,
            contents=messages,
            config=generate_content_config,
        )
        for chunk in response:
            if chunk.function_calls is not None:
                
                # print(YELLOW, chunk, RESET)
                # print(chunk.function_calls)
                # print()
                for call in chunk.function_calls:
                    
                    if call is None:
                        print("THe call is none")
                        continue
                    function_calls.append(call.name)
                    result = tools.execute_tool(call.name, call.args)   
                    raw_messages.append(
                        {                        
                            "role":"model",
                            "content" : f"The result of calling the {call.name} function with parameters ({call.args}) was: {result}."
                        }
                    )
                    if len(raw_messages) > prefs.history_depth:
                        raw_messages = raw_messages[prefs.history_depth // 4:]
                    print("new function call (appended)")

        with open("static_storage/conversation.json", "w", encoding="utf-8") as f:
            f.write(json.dumps(raw_messages, indent=4, ensure_ascii=False))
        return function_calls
    except Exception as e:
        print(RED, e, RESET)
        
    
    
    
def update_context():
    try:
        print("starting context updating")
        current_datetime = datetime.datetime.now(prefs.timezone).strftime('%D-%M-%Y %H:%M:%S %Z')
        
        
            
        system_message = types.Part.from_text(text=f"Current time is {current_datetime} {prefs.system_msg()}")   
        
        with open("static_storage/conversation.json", "r", encoding="utf-8") as f:
            conversation = json.load(f)
        
        query = types.Content(
            role="model",
            parts=[
                types.Part.from_text(text=f"""
–ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
–û–±–Ω–æ–≤–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–º–Ω–æ–≥–æ.
–í –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å:
    —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –±–µ—Å–µ–¥—ã –∏ –µ–µ –∏—Å—Ç–æ—Ä–∏—è.
    –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å.
    –ø—Ä–∏–º–µ—Ä–Ω—ã–µ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è –æ –Ω–∞–º–µ—Ä–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.
    –°–≤–æ–∏ —Ü–µ–ª–∏ –∏ –Ω–∞–º–µ—Ä–µ–Ω–∏—è, –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏.
    –û–ø–∏—à–∏ —Å–≤–æ–∏ –∂–µ–ª–∞–Ω–∏—è.
    
–û–±–Ω–æ–≤–ª—è–π, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π. –î–ª—è –¥–∞–≤–Ω–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π —É–º–µ–Ω—å—à–∞–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é –æ–ø–∏—Å–∞–Ω–∏—è, –∞ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º - —É–¥–∞–ª–∞–π. –ù—É–∂–Ω–æ –Ω–µ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –∞ –æ–±–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ—Å–∫—Ç. –ù–µ –¥–æ–±–∞–≤–ª—è–π –æ—Ç–≤–µ—Ç –≤ –∫–æ–Ω—Ü–µ. –ù–µ –æ—Å—Ç–∞–≤–ª—è–π –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ–æ—Å—Ç–∞—Ä—Ç–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –°–ª–µ–¥–∏ –∑–∞ –≤—Ä–µ–º–µ–Ω–µ–º –∏ –∏–Ω–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–π –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–º—Ç–∫–∏ –∫ –∑–∞–ø–∏—Å—è–º. –ì—Ä—É–ø–ø–∏—Ä—É–π –∑–∞–ø–∏—Å–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏.

[–∏—Å—Ç–æ—Ä–∏—è –¥–µ–π—Å—Ç–≤–∏–π]
{conversation[len(conversation) // 5:]}
            """),
            ],
        )
        
        generate_content_config = types.GenerateContentConfig(
            temperature=prefs.TEMPERATURE,
            top_p=0.9,
            top_k=40,
            max_output_tokens=10000,
            response_mime_type="text/plain",
            system_instruction=[
                system_message
            ],
        )
        plain_text = ""
        for chunk in client.models.generate_content_stream(
            model=prefs.model_gemini,
            contents=[query],
            config=generate_content_config,
        ):
            if not chunk.function_calls:
                plain_text += chunk.text
           
        with open("static_storage/context.txt", "w", encoding="utf-8") as f:
            f.write(plain_text)

        
        print("context updated")
    except Exception as e:
        print(RED, e , RESET)
        error_msg = f"Context_failure {str(e )}"
        bot.send_message(prefs.TST_chat_id, f"üî¥\n```{error_msg}```", parse_mode="Markdown")
        return 1
    