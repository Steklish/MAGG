import time
import prefs
from bot_instance import client, bot
import datetime
from stuff import *
import json
from stuff import *
from google.genai import types
import tools_package.tools as tools


def convert_conversation():
    google_conversation = []
    with open("static_storage/conversation.json", "r", encoding="utf-8") as f:
        conversation = json.load(f)
    for turn in conversation:
        google_conversation.append(types.Content(
            role=turn["role"],
            parts=[
                types.Part.from_text(text=turn["content"]),
            ]
        ))
    return google_conversation


def convert_single(message:str):
    return types.Content(
            role="user",
            parts=[
                types.Part.from_text(text=message),
            ]
        )


def convert_single_as_function(message:str):
    return types.Content(
            role="model",
            parts=[
                types.Part.from_text(text=message),
            ]
        )
        

def raw_conversation():
    with open("static_storage/conversation.json", "r", encoding="utf-8") as f:
        conversation = json.load(f)
    return conversation



def smart_response(
    TOOLSET=tools.G_TOOLS, 
    TEMP=prefs.TEMPERATURE, 
    system_message:types.Part=None, 
    func_mode="AUTO",
    messages=None
):
    """
    Args:
        func_mode: The mode for function calling. Must be one of "ANY", "AUTO", or "NONE".
                    - "ANY": Forces the model to predict only function calls.
                    - "AUTO": Lets the model decide between function calls and text responses.
                    - "NONE": Disables function calling entirely.
                    Defaults to "AUTO".
"""
    
    try:
        print(f"{GREEN}Smart message launched{RESET}")
        current_datetime = datetime.datetime.now(prefs.timezone).strftime('%D-%M-%Y %H:%M:%S %Z')
        if not system_message:
            system_message = types.Part.from_text(text=f"Current time is {current_datetime} {prefs.system_msg() or ''}")   
            
        print("sys message created")
        
        tool_config = types.ToolConfig(
            function_calling_config=types.FunctionCallingConfig(
                mode=func_mode,
                # allowed_function_names=["get_current_weather"],
            )
        )
        print("tool config created")
        
        generate_content_config = types.GenerateContentConfig(
            temperature=TEMP,
            top_p=0.95,
            top_k=40,
            max_output_tokens=8192,
            tools=TOOLSET,
            response_mime_type="text/plain",
            system_instruction=[
                system_message
            ],
            tool_config=tool_config
        )
        print("config created")
        if messages is None:
            messages = convert_conversation()
            
            
        raw_messages = raw_conversation()
        print("messages loaded")
        
        plain_text = ""
        function_calls = []
        response = client.models.generate_content_stream(
            model=prefs.model_gemini,
            contents=messages,
            config=generate_content_config,
        )
        for chunk in response:
            if chunk.function_calls is not None:
                
                # print(YELLOW, chunk, RESET)
                # print(chunk.function_calls)
                # print()
                for call in chunk.function_calls:
                    
                    if call is None:
                        print("The call is none")
                        continue
                    function_calls.append(call.name)
                    # Start the timer
                    start_time = time.time()

                    # Call the function
                    result = tools.execute_tool(call.name, call.args)

                    # End the timer
                    end_time = time.time()

                    # Calculate the elapsed time in seconds
                    execution_time = end_time - start_time 
                    if call.name != "send_message":
                        log_message_with_sender(result, f"function{call.name} ({call.args})", "MAGG")
                    raw_messages.append(
                        {                        
                            "role":"model",
                            "content" : f"The result of calling the {call.name} function with parameters ({call.args}) was: {result}.  [additional info] execution time {execution_time} s. "
                        }
                    )
                    if len(raw_messages) > prefs.history_depth:
                        raw_messages = raw_messages[prefs.history_depth // 4:]
                    print("new function call (appended)")

        with open("static_storage/conversation.json", "w", encoding="utf-8") as f:
            f.write(json.dumps(raw_messages, indent=4, ensure_ascii=False))
        return function_calls
    except Exception as e:
        print(RED, e, RESET)
        
    
    
    
def update_context():
    try:
        print("starting context updating")
        current_datetime = datetime.datetime.now(prefs.timezone).strftime('%D-%M-%Y %H:%M:%S %Z')
        
        
            
        system_message = types.Part.from_text(text=f"Current time is {current_datetime} {prefs.system_msg() or ''}")   
        
        with open("static_storage/conversation.json", "r", encoding="utf-8") as f:
            conversation = json.load(f)
        
        query = types.Content(
            role="model",
            parts=[
                types.Part.from_text(text=f"""
–ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
–û–±–Ω–æ–≤–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ–º–Ω–æ–≥–æ.
–í –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å:
    —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –±–µ—Å–µ–¥—ã –∏ –µ–µ –∏—Å—Ç–æ—Ä–∏—è.
    –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π.
    –ø—Ä–∏–º–µ—Ä–Ω—ã–µ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è –æ –Ω–∞–º–µ—Ä–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.
    –°–≤–æ–∏ —Ü–µ–ª–∏ –∏ –Ω–∞–º–µ—Ä–µ–Ω–∏—è, –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏.
    –û–ø–∏—à–∏ —Å–≤–æ–∏ –∂–µ–ª–∞–Ω–∏—è.
    –û–ø–∏—à–∏, –∫–∞–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω—É–¥–Ω–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –±–µ—Å–µ–¥—ã –∏ –∏–º–µ–µ—Ç—Å—è –ª–∏ –æ–Ω–∞ —É —Ç–µ–±—è. –≠—Ç–æ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ.
    –°–æ—Ö—Ä–∞–Ω—è–π –≤–∞–∂–Ω—É—é —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
    
–û–±–Ω–æ–≤–ª—è–π, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π. –†–∞–∑–¥–µ–ª–∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Ç–µ–∫—É—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –î–ª—è –¥–∞–≤–Ω–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π —É–º–µ–Ω—å—à–∞–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é –æ–ø–∏—Å–∞–Ω–∏—è, –∞ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º - —É–¥–∞–ª—è–π. –ù—É–∂–Ω–æ –Ω–µ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ, –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –æ–±–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç. –°–ª–µ–¥–∏ –∑–∞ –≤—Ä–µ–º–µ–Ω–µ–º –∏ –∏–Ω–æ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–π –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–º–µ—Ç–∫–∏ –∫ –∑–∞–ø–∏—Å—è–º. –ì—Ä—É–ø–ø–∏—Ä—É–π –∑–∞–ø–∏—Å–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏. –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–Ω—è–π —Ä–µ–∂–µ, —á–µ–º —Ç–µ–∫—É—â–∏–π. –¢–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–Ω—è–π —á–∞—â–µ.

[–∏—Å—Ç–æ—Ä–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π]
{conversation[len(conversation) // 5:]}
            """),
            ],
        )
        
        generate_content_config = types.GenerateContentConfig(
            temperature=prefs.TEMPERATURE,
            top_p=0.9,
            top_k=40,
            max_output_tokens=10000,
            response_mime_type="text/plain",
            system_instruction=[
                system_message
            ],
        )
        plain_text = ""
        for chunk in client.models.generate_content_stream(
            model=prefs.model_gemini,
            contents=[query],
            config=generate_content_config,
        ):
            if not chunk.function_calls:
                plain_text += str(chunk.text)
           
        with open("static_storage/context.txt", "w", encoding="utf-8") as f:
            f.write(plain_text)

        print("context updated")
    except Exception as e:
        print(RED, e , RESET)
        error_msg = f"Context_failure {str(e)}"
        bot.send_message(prefs.TST_chat_id, f"üî¥\n```{error_msg}```", parse_mode="Markdown")
        return 1



def summarize_file(filename: str):
    try:
        with open(filename, "r", encoding="utf-8") as f:
            content = f.read()

        system_message = types.Part.from_text(text=f"[character] {prefs.system_msg_char} [context] {prefs.get_context()}")
        query = types.Content(
            role="user",
            parts=[
                types.Part.from_text(text=f"Summarize todays message history:\n{content}"),
            ],
        )

        generate_content_config = types.GenerateContentConfig(
            temperature=0.3,
            top_p=0.9,
            top_k=40,
            max_output_tokens=1000,
            response_mime_type="text/plain",
            system_instruction=[system_message]
        )

        summary = ""
        for chunk in client.models.generate_content_stream(
            model=prefs.model_gemini,
            contents=[query],
            config=generate_content_config,
        ):
            if not chunk.function_calls:
                summary += str(chunk.text)

        timestamp = (datetime.datetime.now(prefs.timezone) - datetime.timedelta(hours=1)).strftime('%Y-%m-%d %H:%M:%S %Z')
        memory_entry = {
            "date created": timestamp,
            "content": summary
        }

        try:
            with open("static_storage/long_term_memory.json", "r", encoding="utf-8") as f:
                memories = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return

        memories.append(memory_entry)
        with open("static_storage/long_term_memory.json", "w", encoding="utf-8") as f:
            json.dump(memories, f, indent=4, ensure_ascii=False)

        return summary
    except Exception as e:
        print(RED, f"Summarization error: {e}", RESET)
        return None